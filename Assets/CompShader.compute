// Each #kernel tells which function to compile; you can have many kernels
#pragma kernel validate
#pragma kernel record
#pragma kernel run
#pragma kernel runField
#pragma kernel recordField

// Create a RenderTexture with enableRandomWrite flag and set it
// with cs.SetTexture

struct Node {
	float3 pos;
	float force;
};

struct Particle {
	float3 position;
	float3 velocity;
};

StructuredBuffer<Particle> _ParticleDataIn;
RWStructuredBuffer<Particle> _ParticleDataOut;

RWStructuredBuffer<float3> _RecordedData;
RWStructuredBuffer<float> _SumData;

RWStructuredBuffer<Node> _Nodes;
RWStructuredBuffer<Node> _BestNodes;
StructuredBuffer<Node> _DeltaNodes;

#define thread_group_size_x 16
#define thread_group_size_y 16

Texture2DArray<float2> _VelTexture;

static float3 g = float3(-0.0000918, 0.0, 0.0);

float deltaTime;
int _particleCount;
unsigned int _maxIterations;
unsigned int _dimensionWidth; 
unsigned int _validationStepSize;
float _deltaScale;
float _gradientScale;
int _iteration;

groupshared float2 sdata[thread_group_size_x*thread_group_size_y];
void reduce(uint gIdx : SV_GroupIndex) {
	GroupMemoryBarrierWithGroupSync();
	// do reduction in shared mem
	for (unsigned int s = 1; s < thread_group_size_x*thread_group_size_y; s *= 2) {
		if (gIdx % (2 * s) == 0) //when s == 1, every second element, when s == 2, every fourth element, etc. s stands for step and stepsize.
		{
			sdata[gIdx] += sdata[gIdx + s];
		}
		GroupMemoryBarrierWithGroupSync();
	}
}

float3 getDeltaVelocityNode(float3 position, float3 node_position, float node_force) {
	float distanceToNode = max(2.0, distance(node_position, position));
	float3 deltaVelocity = normalize(node_position - position) * node_force / pow(distanceToNode, 1.7);
	return deltaVelocity;
}


[numthreads(thread_group_size_x, thread_group_size_y, 1)]
void validate(uint gIdx : SV_GroupIndex)
{
	//int idx = id.x + id.y * 512; //thread_group_size_x * 32;
	const unsigned int idx = gIdx;//(id.x/* % _dimensionWidth*/) + _dimensionWidth * (id.y /*% _dimensionWidth*/); //(id.y + 1 * id.z);

	float3 positionPos = _ParticleDataIn[idx].position;
	float3 velocityPos = _ParticleDataIn[idx].velocity;

	float3 positionNeg = _ParticleDataIn[idx].position;
	float3 velocityNeg = _ParticleDataIn[idx].velocity;

	float2 errorAccumulated = float2(0, 0);

	for (unsigned int iter = 0; iter < _maxIterations; iter++) {
		float3 recordedPosition = float3(0.0, 0.0, 0.0);
		bool compare = iter % _validationStepSize == 0;
		if (compare) {
			const unsigned int recordingIndex = idx + (iter * _particleCount / _validationStepSize);
			recordedPosition = _RecordedData[recordingIndex];
		}

		float3 sum_velPos = float3(0, 0, 0);
		float3 sum_velNeg = float3(0, 0, 0);
		for (int i = 0; i < 3; i++) {
			float3 node_position = _Nodes[i].pos;
			float node_force = _Nodes[i].force;

			float3 node_positionPos = node_position + _DeltaNodes[i].pos * _deltaScale * 1000.0;
			float node_forcePos = node_force + _DeltaNodes[i].force * _deltaScale;
			float3 deltaVelocityPos = getDeltaVelocityNode(positionPos, node_positionPos, node_forcePos);
			sum_velPos += deltaVelocityPos;

			float3 node_positionNeg = node_position - _DeltaNodes[i].pos * _deltaScale * 1000.0;
			float node_forceNeg = node_force - _DeltaNodes[i].force * _deltaScale;
			float3 deltaVelocityNeg = getDeltaVelocityNode(positionNeg, node_positionNeg, node_forceNeg);
			sum_velNeg += deltaVelocityNeg;
		}

		velocityPos += (sum_velPos / 6.0) * 0.5;
		velocityPos += g;
		float damping = 0.995;
		velocityPos *= damping;
		positionPos += velocityPos;

		velocityNeg += (sum_velNeg / 6.0) * 0.5;
		velocityNeg += g;
		velocityNeg *= damping;
		positionNeg += velocityNeg;
		
		if (compare) {
			float2 errorDistances = float2(distance(positionPos, recordedPosition), distance(positionNeg, recordedPosition));
			errorAccumulated += errorDistances;
		}
	}
	sdata[idx].x = errorAccumulated.x;
	sdata[idx].y = errorAccumulated.x - errorAccumulated.y;
	reduce(gIdx);

	if (gIdx == 0) {
		//The reduced sum is in sdata[0] after the reduce call.
		float errorPos = sdata[0].x;
		float errorDiff = sdata[0].y;
		_SumData[0] = min(errorPos, _SumData[0]);    //Almost no measurable performance penalty for writing to this global memory

		for (int i = 0; i < 3; i++) {
			float gradientForce = errorDiff / (2.0f * _DeltaNodes[i].force) /*+ 0.000001*/; //add small value to avoid division by zero
			float3 gradientPos = errorDiff / (2.0f * _DeltaNodes[i].pos) /*+ 0.000001*/; //add small value to avoid division by zero
			_Nodes[i].force -= gradientForce * _gradientScale;
			_Nodes[i].pos -= gradientPos * _gradientScale * 100.0f;
		}
	}

	//GroupMemoryBarrierWithGroupSync();
	//Here would should somehow get the true error with the current node configuration, so that we can find the config with the lowest error.
}

[numthreads(thread_group_size_x, thread_group_size_y, 1)]
void record(uint3 id : SV_DispatchThreadID)
{
	//int idx = id.x + id.y * 512; //thread_group_size_x * 32;
	const unsigned int idx = id.x + _dimensionWidth * id.y; //(id.y + 1 * id.z);

	float3 position = _ParticleDataIn[idx].position;
	float3 velocity = _ParticleDataIn[idx].velocity;

	for (unsigned int iter = 0; iter < _maxIterations; iter++) {
		float3 sum_vel = float3(0, 0, 0);
		for (int i = 0; i < 3; i++) {
			float3 deltaVelocity = getDeltaVelocityNode(position, _Nodes[i].pos, _Nodes[i].force);
			sum_vel = sum_vel + deltaVelocity;
		}

		velocity += (sum_vel / 6.0) * 0.5;
		velocity += g;
		float damping = 0.995;
		velocity = velocity * damping;

		position = position + velocity;

		if (iter % _validationStepSize == 0) {
			_RecordedData[idx + (iter * _particleCount / _validationStepSize)] = position;
		}
	}
}

[numthreads(thread_group_size_x, thread_group_size_y, 1)]
void run(uint3 id : SV_DispatchThreadID)
{
	//int idx = id.x + id.y * 512; //thread_group_size_x * 32;
	const unsigned int idx = id.x + _dimensionWidth * id.y; //(id.y + 1 * id.z);

	float3 position = _ParticleDataIn[idx].position;
	float3 velocity = _ParticleDataIn[idx].velocity;

	float3 sum_vel = float3(0, 0, 0);
	//these operations could be combined into matrix operations up to size 4x4. This may increase performance a bit.
	for (unsigned int i = 0; i < 3; i++) {
		float3 deltaVelocity = getDeltaVelocityNode(position, _Nodes[i].pos, _Nodes[i].force);
		sum_vel = sum_vel + deltaVelocity;
	}

	velocity += (sum_vel / 6.0) * 0.5;
	velocity += g;
	float damping = 0.995;
	velocity = velocity * damping;

	position += velocity;
	_ParticleDataOut[idx].position = position;
	_ParticleDataOut[idx].velocity = velocity;
	
}

[numthreads(thread_group_size_x, thread_group_size_y, 1)]
void runField(uint3 id : SV_DispatchThreadID)
{
	if (_iteration < 172 * 5) {
		const unsigned int idx = id.x + _dimensionWidth * id.y;

		float3 position = _ParticleDataIn[idx].position;
		//float3 velocity = _ParticleDataIn[idx].velocity;

		float2 coordPosition = position.xy;
		//coordPosition.x -= 20.0;
		if (coordPosition.x >= 0 || coordPosition.x <= 63.0 && coordPosition.y >= 0 || coordPosition.y <= 63.0) {
			uint3 samplingCoords = uint3(coordPosition + 32.0, _iteration / 5);  // need to scale this correctly

			float2 sampledVelocity = _VelTexture[samplingCoords];
			position.xy += sampledVelocity;

			_ParticleDataOut[idx].position = position;
			_ParticleDataOut[idx].velocity = float3(sampledVelocity, 0);
		}
	}
}

[numthreads(thread_group_size_x, thread_group_size_y, 1)]
void recordField(uint3 id : SV_DispatchThreadID)
{
	//int idx = id.x + id.y * 512; //thread_group_size_x * 32;
	const unsigned int idx = id.x + _dimensionWidth * id.y; //(id.y + 1 * id.z);

	float3 position = _ParticleDataIn[idx].position;


	for (unsigned int iter = 0; iter < _maxIterations; iter++) {
		float2 coordPosition = position.xy;

		if (coordPosition.x >= 0 || coordPosition.x <= 63.0 && coordPosition.y >= 0 || coordPosition.y <= 63.0) {

			uint3 samplingCoords = uint3(coordPosition + 32.0, iter / 5);

			float2 velocity = _VelTexture[samplingCoords];
			position.xy += velocity;
		}
	
		if (iter % _validationStepSize == 0) {
			_RecordedData[idx + (iter * _particleCount / _validationStepSize)] = position;
		}
	}
}