// Each #kernel tells which function to compile; you can have many kernels
//#pragma kernel reduce1
#pragma kernel myReduce

//From Nvidia DirectCompute Optimizations and best Practices 2010:
// http://on-demand.gputechconf.com/gtc/2010/presentations/S12312-DirectCompute-Pre-Conference-Tutorial.pdf

/*RWStructuredBuffer<float> g_data;
#define groupDim_x 256
groupshared float sdata[groupDim_x];
[numthreads(groupDim_x, 1, 1)]
void reduce1(//uint3 id : SV_DispatchThreadID,
		      uint3	threadIdx	: SV_GroupThreadID,
			  uint3 groupIdx	: SV_GroupID)
{
	// each thread loads one element from global to shared mem
	unsigned int tid = threadIdx.x;
	unsigned int i = groupIdx.x	* groupDim_x + threadIdx.x;
	sdata[tid] = g_data[i];
	GroupMemoryBarrierWithGroupSync();
	// do reduction in shared mem
	for (unsigned int s = 1; s < groupDim_x; s *= 2) {
		if(tid % (2 * s) == 0)
		{
			sdata[tid] += sdata[tid + s];
		}
		GroupMemoryBarrierWithGroupSync();
	}
	// write result for this block to global mem
	if (tid == 0) {
		g_data[groupIdx.x] = sdata[0];
	}
}*/

StructuredBuffer<float> g_data;
RWStructuredBuffer<float> _SumData;
#define groupDim_x 16
groupshared float sdata[groupDim_x*groupDim_x];
[numthreads(groupDim_x, groupDim_x, 1)]
void myReduce(uint3 id : SV_DispatchThreadID, 
			  uint3	threadIdx : SV_GroupThreadID,
			  uint	gIdx : SV_GroupIndex,
			  uint3 groupIdx : SV_GroupID) {

	const unsigned int idx = id.x + groupDim_x * id.y;

	unsigned int tid = gIdx;//threadIdx.x + groupDim_x * threadIdx.y; //threadIdx is the id within the group
	unsigned int i = groupIdx.x	* groupDim_x + threadIdx.x + groupDim_x * threadIdx.y;

	sdata[tid] = g_data[i];
	GroupMemoryBarrierWithGroupSync();
	// do reduction in shared mem
	for (unsigned int s = 1; s < groupDim_x*groupDim_x; s *= 2) {
		if (tid % (2 * s) == 0) //when s == 1, every second element, when s == 2, every fourth element, etc. s stands for step and stepsize.
		{
			sdata[tid] += sdata[tid + s];
		}
		GroupMemoryBarrierWithGroupSync();
	}
	// write result for this block to global mem
	if (tid == 0) {
		_SumData[0] = sdata[0];
	}
}


/*cbuffer consts{
	uint n;
	uint dispatchDim_x;
};
groupshared float sdata[groupDim_x];
[numthreads(groupDim_x, 1, 1)] void reduce1(uint tid : SV_GroupIndex, uint3 groupIdx : SV_GroupID) {
	unsigned int i = groupIdx.x * (groupDim_x * 2) + tid;
	unsigned int dispatchSize = groupDim_x * 2 * 1024;//dispatchDim_x;
	sdata[tid] = 0;

	do { 
		sdata[tid] += g_idata[i] + g_idata[i + groupDim_x]; 
		i += dispatchSize; 
	} while (i < 3);

	GroupMemoryBarrierWithGroupSync();

	if (groupDim_x >= 512) {
		if (tid < 256) {
			sdata[tid] += sdata[tid + 256];
		}
		GroupMemoryBarrierWithGroupSync();
	}
	if (groupDim_x >= 256) {
		if (tid < 128) {
			sdata[tid] += sdata[tid + 128];
		}
		GroupMemoryBarrierWithGroupSync();
	}
	if (groupDim_x >= 128) {
		if (tid < 64) {
			sdata[tid] += sdata[tid + 64];
		}
		GroupMemoryBarrierWithGroupSync();
	}
	if (tid < 32) {
		if (groupDim_x >= 64)
			sdata[tid] += sdata[tid + 32];
		if (groupDim_x >= 32)
			sdata[tid] += sdata[tid + 16];
		if (groupDim_x >= 16)
			sdata[tid] += sdata[tid + 8];
		if (groupDim_x >= 8)
			sdata[tid] += sdata[tid + 4];
		if (groupDim_x >= 4)
			sdata[tid] += sdata[tid + 2];
		if (groupDim_x >= 2)
			sdata[tid] += sdata[tid + 1];
	}
	if (tid == 0)
		g_odata[groupIdx.x] = sdata[0];
}*/